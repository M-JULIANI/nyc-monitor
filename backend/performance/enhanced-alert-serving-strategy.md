# Enhanced Alert Serving Strategy

## Overview

This document outlines the implementation of an enhanced alert serving strategy that enables querying historical data spanning 6+ months while maintaining sub-200ms response times. The strategy combines intelligent backend caching with Redis, NYC-specific spatial grid optimization, and frontend request throttling.

## Problem Statement

The original system was limited to serving 7 days of alert data with response times around 800ms. Users needed access to historical data spanning months for trend analysis and research purposes, but direct database queries for extended date ranges resulted in unacceptable performance degradation.

## Solution Architecture

### Backend: Redis Caching with NYC-Optimized Spatial Grid

The core innovation is a zoom-aware spatial grid caching system specifically optimized for New York City's geographic bounds and typical usage patterns.

#### NYC Geographic Context
```python
# NYC bounding box constants
NYC_BOUNDS = {
    'north': 40.92,   # Northernmost point of NYC
    'south': 40.49,   # Southernmost point of NYC  
    'east': -73.70,   # Easternmost point of NYC
    'west': -74.27    # Westernmost point of NYC
}
```

#### Adaptive Grid Sizing Strategy

The caching system uses different grid cell sizes based on viewport zoom level:

| Zoom Level | Viewport Area | Grid Size | Grid Cell | Use Case |
|------------|---------------|-----------|-----------|----------|
| City-wide | >50% of NYC | 0.02Â° (~2km) | Borough-level | All boroughs view |
| Borough | 10-50% of NYC | 0.01Â° (~1km) | Neighborhood clusters | Manhattan, Brooklyn views |
| Neighborhood | 2-10% of NYC | 0.005Â° (~500m) | Local areas | Upper East Side, Williamsburg |
| Street-level | <2% of NYC | 0.002Â° (~200m) | Precise local | Block-level detail |

#### Cache Key Generation

```python
def get_cache_key(bbox: str, start_date: str, end_date: str) -> str:
    lat1, lng1, lat2, lng2 = map(float, bbox.split(','))
    
    # Handle requests outside NYC with special metro area cache
    if not is_within_nyc(bbox):
        return f"metro_area:{start_date}:{end_date}"
    
    # Calculate viewport size for NYC-specific grid sizing
    bbox_width = abs(lng2 - lng1)
    bbox_height = abs(lat2 - lat1)
    bbox_area = bbox_width * bbox_height
    
    # NYC-optimized grid sizes
    if bbox_area > 0.05:  # City-wide view
        grid_size = 0.02
        cache_prefix = "city"
    elif bbox_area > 0.01:  # Borough view
        grid_size = 0.01
        cache_prefix = "borough"
    elif bbox_area > 0.002:  # Neighborhood view
        grid_size = 0.005
        cache_prefix = "neighborhood" 
    else:  # Street level
        grid_size = 0.002
        cache_prefix = "street"
    
    # Snap viewport to grid boundaries
    lat1_grid = math.floor(lat1 / grid_size) * grid_size
    lng1_grid = math.floor(lng1 / grid_size) * grid_size  
    lat2_grid = math.ceil(lat2 / grid_size) * grid_size
    lng2_grid = math.ceil(lng2 / grid_size) * grid_size
    
    return f"viewport:{cache_prefix}:{grid_size}:{lat1_grid},{lng1_grid},{lat2_grid},{lng2_grid}:{start_date}:{end_date}"
```

#### Benefits of This Approach

1. **Shared Cache Efficiency**: Multiple users viewing overlapping areas benefit from shared cached regions
2. **Persistence**: Cache survives server restarts, reducing cold start penalties
3. **Memory Optimization**: Avoids duplicating data across browser sessions
4. **Zoom-Aware Intelligence**: Adapts grid size to viewport, maximizing cache hit rates
5. **NYC-Specific Optimization**: Tailored to actual geographic usage patterns

### Frontend: Request Throttling and Performance Measurement

#### Debounced Request Strategy

```typescript
const useViewportAlerts = () => {
  const [performanceMetrics, setPerformanceMetrics] = useState<any[]>([]);
  
  // 300ms debounce prevents excessive API calls during map interactions
  const debouncedFetchAlerts = useCallback(
    debounce(async (bbox: BoundingBox, dateRange: DateRange) => {
      const startTime = performance.now();
      
      try {
        const response = await fetch(
          `/api/alerts/viewport?bbox=${bbox.join(',')}&start=${dateRange.start}&end=${dateRange.end}`
        );
        const data = await response.json();
        
        const endTime = performance.now();
        const roundTripTime = endTime - startTime;
        
        // Measure map rendering performance
        const mapUpdateStart = performance.now();
        setAlerts(data.alerts);
        
        // Use requestAnimationFrame to measure actual render completion
        requestAnimationFrame(() => {
          const mapUpdateEnd = performance.now();
          const renderTime = mapUpdateEnd - mapUpdateStart;
          
          // Store comprehensive performance metrics
          const metrics = {
            timestamp: new Date().toISOString(),
            bbox_area: calculateBboxArea(bbox),
            alert_count: data.alerts.length,
            backend_performance: data.performance,
            frontend_metrics: {
              round_trip_time_ms: Math.round(roundTripTime),
              render_time_ms: Math.round(renderTime),
              total_frontend_time_ms: Math.round(endTime - startTime + renderTime)
            }
          };
          
          setPerformanceMetrics(prev => [...prev.slice(-19), metrics]);
        });
        
      } catch (error) {
        console.error('Viewport fetch failed:', error);
      }
    }, 300), // 300ms debounce window
    []
  );
  
  return { debouncedFetchAlerts, performanceMetrics };
};
```

#### Why 300ms Debounce?

- **User Experience**: Allows smooth map panning without stuttering
- **API Efficiency**: Prevents rapid-fire requests during continuous map movement
- **Performance**: Reduces unnecessary cache lookups and database queries
- **Battery Life**: Minimizes network activity on mobile devices

## Performance Measurement Strategy

### Backend Metrics

The API endpoint includes comprehensive performance instrumentation:

```python
@alerts_router.get('/viewport')
async def get_viewport_alerts(
    bbox: str, start_date: str, end_date: str, limit: int = 2000
):
    start_time = time.time()
    cache_start = time.time()
    
    cache_key = get_cache_key(bbox, start_date, end_date)
    cached = redis_client.get(cache_key)
    cache_time = time.time() - cache_start
    
    if cached:
        total_time = time.time() - start_time
        return {
            'alerts': json.loads(cached),
            'performance': {
                'total_time_ms': round(total_time * 1000, 2),
                'cache_hit': True,
                'cache_time_ms': round(cache_time * 1000, 2),
                'alert_count': len(json.loads(cached)),
                'source': 'redis_cache'
            }
        }
    
    # Database query with timing
    db_start = time.time()
    alerts = query_alerts_in_bbox_and_daterange(bbox, start_date, end_date, limit)
    db_time = time.time() - db_start
    
    # Cache write timing
    cache_write_start = time.time()
    ttl = 3600 if is_recent_data(start_date) else 86400
    redis_client.setex(cache_key, ttl, json.dumps(alerts))
    cache_write_time = time.time() - cache_write_start
    
    total_time = time.time() - start_time
    
    return {
        'alerts': alerts,
        'performance': {
            'total_time_ms': round(total_time * 1000, 2),
            'cache_hit': False,
            'db_query_time_ms': round(db_time * 1000, 2),
            'cache_write_time_ms': round(cache_write_time * 1000, 2),
            'alert_count': len(alerts),
            'source': 'database_fresh'
        }
    }
```

### Key Performance Indicators

#### Backend Metrics
- **Cache Hit Rate**: Target >80% after initial cache warming
- **Database Query Time**: Target <500ms for complex spatial queries
- **Cache Lookup Time**: Target <10ms for Redis operations
- **Total API Response Time**: 
  - Cache hits: <100ms
  - Cache misses: <600ms

#### Frontend Metrics  
- **Round-trip Time**: Network latency + backend processing
- **Map Render Time**: Time for Mapbox to update visual display
- **Total User-Perceived Latency**: From map interaction to visual completion

### Success Criteria

The implementation is considered successful when:

1. **Cache Performance**: >75% hit rate with consistent <200ms response times
2. **Historical Data Access**: Can serve 30+ days of data with <1s total latency
3. **Map Responsiveness**: Smooth interactions with no stuttering during pan/zoom
4. **Resource Efficiency**: Reasonable Redis memory usage with predictable growth
5. **Scalability**: Performance maintains under concurrent user load

## Special Handling: Metro Area Requests

For viewports extending beyond NYC boundaries, the system employs a special "metro area" strategy:

```python
def get_metro_area_highlights(start_date: str, end_date: str, limit: int = 100):
    """Return high-priority alerts for metro area view"""
    return query_alerts_filtered(
        bbox=f"{NYC_BOUNDS['south']},{NYC_BOUNDS['west']},{NYC_BOUNDS['north']},{NYC_BOUNDS['east']}",
        start_date=start_date,
        end_date=end_date, 
        min_severity=7,  # Only critical alerts for wide views
        limit=limit
    )
```

This approach:
- Provides meaningful data when users zoom out beyond NYC
- Prevents overwhelming wide-view requests with excessive data
- Maintains performance by limiting result sets
- Offers a curated "highlight reel" of critical incidents

## Implementation Timeline

### Phase 1: Backend Infrastructure
1. Implement NYC-aware cache key generation
2. Add Redis caching layer to viewport endpoint
3. Implement performance instrumentation
4. Add metro area fallback handling

### Phase 2: Frontend Optimization  
1. Implement request debouncing
2. Add performance measurement hooks
3. Create performance monitoring dashboard
4. Optimize map rendering pipeline

### Phase 3: Monitoring and Tuning
1. Deploy performance measurement system
2. Analyze cache hit patterns and optimize grid sizes
3. Fine-tune TTL values based on usage patterns
4. Implement automated performance alerts

## Expected Outcomes

### Performance Improvements
- **Response Time**: From 800ms average to 150ms average
- **Data Range**: From 7 days maximum to 6+ months
- **Cache Efficiency**: >80% hit rate for common NYC areas
- **User Experience**: Smooth map interactions with sub-second data loading

### Demo Value
The implementation provides a compelling narrative:
1. **Problem**: "Limited to 7 days of data with 800ms response times"
2. **Solution**: "Intelligent spatial caching with NYC optimization"  
3. **Result**: "6 months of data with 150ms average response times"
4. **Innovation**: "Zoom-aware grid system adapts to user behavior"

## Testing and Validation

### Performance Testing Framework

To validate the effectiveness of the enhanced caching strategy, we implement comprehensive testing that measures both response times and cache efficiency across different scenarios.

#### Backend Performance Testing Script

```javascript
const performanceTest = async () => {
  const testCases = [
    { 
      name: 'current_7_days', 
      endpoint: '/api/alerts/recent?hours=168',
      description: 'Legacy endpoint for comparison'
    },
    { 
      name: 'viewport_7_days', 
      endpoint: '/api/alerts/viewport?bbox=40.7,-74.0,40.8,-73.9&start=2024-01-01&end=2024-01-07',
      description: 'New caching system - 1 week Manhattan'
    },
    { 
      name: 'viewport_30_days', 
      endpoint: '/api/alerts/viewport?bbox=40.7,-74.0,40.8,-73.9&start=2023-12-01&end=2024-01-01',
      description: 'Extended historical data - 1 month Manhattan'
    },
    {
      name: 'viewport_6_months',
      endpoint: '/api/alerts/viewport?bbox=40.7,-74.0,40.8,-73.9&start=2023-07-01&end=2024-01-01',
      description: 'Full historical range - 6 months Manhattan'
    },
    {
      name: 'borough_level',
      endpoint: '/api/alerts/viewport?bbox=40.6,-74.1,40.9,-73.8&start=2024-01-01&end=2024-01-07',
      description: 'Borough-level caching test'
    },
    {
      name: 'street_level',
      endpoint: '/api/alerts/viewport?bbox=40.75,-73.99,40.76,-73.98&start=2024-01-01&end=2024-01-07',
      description: 'Street-level precision caching'
    },
    {
      name: 'metro_area',
      endpoint: '/api/alerts/viewport?bbox=40.0,-75.0,41.0,-73.0&start=2024-01-01&end=2024-01-07',
      description: 'Metro area fallback handling'
    }
  ];
  
  const results = {};
  
  for (const test of testCases) {
    console.log(`\nðŸ§ª Testing: ${test.name} - ${test.description}`);
    
    const coldTimes = [];
    const warmTimes = [];
    const cacheHitData = [];
    
    // Test cache misses (cold cache)
    for (let i = 0; i < 3; i++) {
      await fetch('/api/cache/clear'); // Clear cache for cold start
      await new Promise(resolve => setTimeout(resolve, 100));
      
      const start = performance.now();
      const response = await fetch(test.endpoint);
      const data = await response.json();
      const responseTime = performance.now() - start;
      
      coldTimes.push(responseTime);
      cacheHitData.push({
        run: i + 1,
        cache_hit: data.performance?.cache_hit || false,
        response_time: responseTime,
        alert_count: data.alerts?.length || 0,
        source: data.performance?.source || 'unknown'
      });
    }
    
    // Test cache hits (warm cache)
    for (let i = 0; i < 5; i++) {
      const start = performance.now();
      const response = await fetch(test.endpoint);
      const data = await response.json();
      const responseTime = performance.now() - start;
      
      warmTimes.push(responseTime);
      cacheHitData.push({
        run: i + 4,
        cache_hit: data.performance?.cache_hit || false,
        response_time: responseTime,
        alert_count: data.alerts?.length || 0,
        source: data.performance?.source || 'unknown'
      });
    }
    
    // Calculate statistics
    const avgColdTime = Math.round(coldTimes.reduce((a,b) => a+b) / coldTimes.length);
    const avgWarmTime = Math.round(warmTimes.reduce((a,b) => a+b) / warmTimes.length);
    const cacheHitRate = (cacheHitData.filter(d => d.cache_hit).length / cacheHitData.length) * 100;
    const improvementPercent = Math.round(((avgColdTime - avgWarmTime) / avgColdTime) * 100);
    
    results[test.name] = {
      description: test.description,
      cold_cache_avg_ms: avgColdTime,
      warm_cache_avg_ms: avgWarmTime,
      cache_hit_rate_percent: Math.round(cacheHitRate),
      performance_improvement_percent: improvementPercent,
      sample_alert_count: cacheHitData[0]?.alert_count || 0,
      raw_data: cacheHitData
    };
    
    console.log(`   Cold cache: ${avgColdTime}ms`);
    console.log(`   Warm cache: ${avgWarmTime}ms`);
    console.log(`   Cache hit rate: ${Math.round(cacheHitRate)}%`);
    console.log(`   Performance improvement: ${improvementPercent}%`);
    console.log(`   Alert count: ${cacheHitData[0]?.alert_count || 0}`);
  }
  
  return results;
};

// Cache Hit Pattern Analysis
const analyzeCachePatterns = async () => {
  console.log('\nðŸ“Š Analyzing Cache Hit Patterns...');
  
  const scenarios = [
    // Test overlapping viewports
    { bbox: '40.75,-74.0,40.76,-73.99', name: 'viewport_a' },
    { bbox: '40.755,-74.005,40.765,-73.995', name: 'viewport_b_overlap' },
    { bbox: '40.76,-74.01,40.77,-74.00', name: 'viewport_c_adjacent' },
    // Test zoom level transitions
    { bbox: '40.7,-74.1,40.8,-73.9', name: 'zoom_out_borough' },
    { bbox: '40.74,-74.02,40.78,-73.98', name: 'zoom_in_neighborhood' },
    { bbox: '40.755,-74.005,40.765,-73.995', name: 'zoom_in_street' }
  ];
  
  for (const scenario of scenarios) {
    const endpoint = `/api/alerts/viewport?bbox=${scenario.bbox}&start=2024-01-01&end=2024-01-07`;
    
    // First request (should be cache miss)
    const response1 = await fetch(endpoint);
    const data1 = await response1.json();
    
    // Second request (should be cache hit)
    const response2 = await fetch(endpoint);
    const data2 = await response2.json();
    
    console.log(`${scenario.name}: Hit=${data2.performance?.cache_hit}, Grid=${data2.performance?.grid_size || 'N/A'}`);
  }
};

// Load Testing Simulation
const loadTest = async (concurrentUsers = 10, duration = 30000) => {
  console.log(`\nâš¡ Load Testing: ${concurrentUsers} concurrent users for ${duration/1000}s`);
  
  const startTime = Date.now();
  const results = [];
  const promises = [];
  
  for (let user = 0; user < concurrentUsers; user++) {
    const userPromise = (async () => {
      const userResults = [];
      
      while (Date.now() - startTime < duration) {
        // Simulate different user behaviors
        const scenarios = [
          '40.75,-74.0,40.76,-73.99',  // Street level
          '40.7,-74.1,40.8,-73.9',     // Borough level  
          '40.6,-74.2,40.9,-73.7',     // City wide
        ];
        
        const bbox = scenarios[Math.floor(Math.random() * scenarios.length)];
        const endpoint = `/api/alerts/viewport?bbox=${bbox}&start=2024-01-01&end=2024-01-07`;
        
        const start = performance.now();
        try {
          const response = await fetch(endpoint);
          const data = await response.json();
          const responseTime = performance.now() - start;
          
          userResults.push({
            user_id: user,
            response_time: responseTime,
            cache_hit: data.performance?.cache_hit || false,
            alert_count: data.alerts?.length || 0,
            timestamp: Date.now()
          });
          
          // Random delay between requests (1-5 seconds)
          await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 4000));
        } catch (error) {
          userResults.push({
            user_id: user,
            error: error.message,
            timestamp: Date.now()
          });
        }
      }
      
      return userResults;
    })();
    
    promises.push(userPromise);
  }
  
  const allResults = await Promise.all(promises);
  const flatResults = allResults.flat();
  
  // Calculate load test statistics
  const successfulRequests = flatResults.filter(r => !r.error);
  const avgResponseTime = successfulRequests.reduce((sum, r) => sum + r.response_time, 0) / successfulRequests.length;
  const cacheHitRate = (successfulRequests.filter(r => r.cache_hit).length / successfulRequests.length) * 100;
  const totalRequests = flatResults.length;
  const errorRate = ((flatResults.length - successfulRequests.length) / flatResults.length) * 100;
  
  console.log(`   Total requests: ${totalRequests}`);
  console.log(`   Average response time: ${Math.round(avgResponseTime)}ms`);
  console.log(`   Cache hit rate: ${Math.round(cacheHitRate)}%`);
  console.log(`   Error rate: ${Math.round(errorRate)}%`);
  
  return {
    total_requests: totalRequests,
    avg_response_time: Math.round(avgResponseTime),
    cache_hit_rate: Math.round(cacheHitRate),
    error_rate: Math.round(errorRate),
    raw_data: flatResults
  };
};

// Main test runner
const runAllTests = async () => {
  console.log('ðŸš€ Starting Enhanced Alert Serving Strategy Performance Tests\n');
  
  const performanceResults = await performanceTest();
  const cachePatterns = await analyzeCachePatterns();
  const loadResults = await loadTest(10, 30000);
  
  console.log('\nðŸ“‹ Test Summary:');
  console.log('================');
  
  // Performance comparison
  const baseline = performanceResults.current_7_days;
  const newSystem = performanceResults.viewport_7_days;
  
  console.log(`Baseline (7 days): ${baseline.cold_cache_avg_ms}ms`);
  console.log(`New system (7 days): ${newSystem.warm_cache_avg_ms}ms`);
  console.log(`Improvement: ${Math.round(((baseline.cold_cache_avg_ms - newSystem.warm_cache_avg_ms) / baseline.cold_cache_avg_ms) * 100)}%`);
  
  // Historical data capability
  const sixMonths = performanceResults.viewport_6_months;
  console.log(`6-month queries: ${sixMonths.warm_cache_avg_ms}ms (${sixMonths.cache_hit_rate_percent}% cache hit rate)`);
  
  return {
    performance: performanceResults,
    load_test: loadResults,
    timestamp: new Date().toISOString()
  };
};
```

#### Test Scenarios and Success Criteria

| Test Scenario | Success Criteria | Expected Outcome |
|---------------|------------------|-------------------|
| **Legacy vs New (7 days)** | New system â‰¥50% faster | 800ms â†’ 150ms |
| **Cache Hit Rate** | >75% after warming | Shared cache efficiency |
| **Historical Data (30+ days)** | <1s total latency | Extended capability |
| **Grid Size Adaptation** | Different cache keys by zoom | Optimized storage |
| **Metro Area Fallback** | Graceful handling | No errors outside NYC |
| **Load Testing (10 users)** | <5% error rate, stable latency | Concurrent scalability |

#### Frontend Integration Testing

```typescript
// Automated frontend performance testing
const frontendPerformanceTest = async () => {
  const testViewports = [
    { name: 'manhattan', bbox: [40.7, -74.0, 40.8, -73.9] },
    { name: 'brooklyn', bbox: [40.6, -74.0, 40.7, -73.9] },
    { name: 'street_level', bbox: [40.75, -73.99, 40.76, -73.98] }
  ];
  
  for (const viewport of testViewports) {
    console.log(`Testing ${viewport.name} viewport...`);
    
    // Simulate map interaction
    const startTime = performance.now();
    
    // Trigger viewport change
    await debouncedFetchAlerts(viewport.bbox, { start: '2024-01-01', end: '2024-01-07' });
    
    // Wait for completion
    await new Promise(resolve => setTimeout(resolve, 1000));
    
    const totalTime = performance.now() - startTime;
    console.log(`${viewport.name}: ${Math.round(totalTime)}ms total interaction time`);
  }
};
```

### Continuous Performance Monitoring

#### Automated Test Pipeline

```bash
# Test runner script
#!/bin/bash
echo "Running Enhanced Alert Serving Strategy Tests..."

# Start backend services
docker-compose up -d redis database

# Wait for services
sleep 10

# Run performance tests
node performance-test.js > test-results-$(date +%Y%m%d-%H%M%S).json

# Generate report
node generate-performance-report.js

echo "Tests complete. Check test-results-*.json for detailed metrics."
```

#### Key Metrics Dashboard

The testing framework should track:

1. **Response Time Distribution**: P50, P95, P99 percentiles
2. **Cache Performance**: Hit rate, miss rate, eviction rate
3. **Error Rates**: By endpoint, by viewport size, by date range
4. **Resource Usage**: Redis memory, CPU utilization, database connections
5. **User Experience**: End-to-end latency, render times, interaction responsiveness

## Technical Considerations

### Redis Memory Management
- Monitor cache size growth patterns
- Implement LRU eviction for historical data
- Consider compression for large alert datasets
- Plan for cache warming strategies

### Database Query Optimization
- Ensure spatial indexes are optimized for bbox queries
- Consider read replicas for cache-miss scenarios  
- Monitor query performance for different date ranges
- Implement query timeout protection

### Error Handling and Resilience
- Graceful degradation when Redis is unavailable
- Circuit breaker pattern for database protection
- Retry logic for transient failures
- Comprehensive logging for debugging

This enhanced alert serving strategy transforms the system from a limited-duration, slow-response service into a comprehensive, high-performance platform capable of supporting extensive historical analysis while maintaining excellent user experience.
